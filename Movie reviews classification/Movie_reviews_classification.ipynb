{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie_reviews_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Working on IMBD movie reviews data"
      ],
      "metadata": {
        "id": "ILh3nnvsPOMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing words as a set: The bag-of-words approach\n",
        "I will use bag-of-words model"
      ],
      "metadata": {
        "id": "7W0RmWBNPSa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also try sequence model. But the model in use will give higher accuracy on this dataset"
      ],
      "metadata": {
        "id": "r_GCCrDuPjxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the text vectorization layer**"
      ],
      "metadata": {
        "id": "9Mwwll1FYPsg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKdteiUfOgny"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "\n",
        "# Using make_vocabulary, encode, and decode method of the Vectorizer class:\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "(Resolved Error in the above code cell)\n",
        "\n",
        "I was facing an entirely different error description. But the actual error was something else.\n",
        "According to the error description:\n",
        "this code is not iterable: \"for token in tokens:\" in the make_vocabulary method\n",
        "\n",
        "\n",
        "Actual error:\n",
        "This was the error in the tokenize method\n",
        "Incorrect: \"return text.split\"\n",
        "Correct: \"return text.split()\"\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "mn2ClGJQjyWs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0b9421d1-e5e4-42d1-e536-991f259f050c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n(Resolved Error in the above code cell)\\n\\nI was facing an entirely different error description. But the actual error was something else.\\nAccording to the error description:\\nthis code is not iterable: \"for token in tokens:\" in the make_vocabulary method\\n\\n\\nActual error:\\nThis was the error in the tokenize method\\nIncorrect: \"return text.split\"\\nCorrect: \"return text.split()\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "id": "-KQawqg-O1Ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7b3529-cf70-479b-aaed-3ce334c270bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "id": "iFJQRWB1u5dW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691a099b-cab1-4630-d3e4-dd778dd3678e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretty amazing result!\n",
        "# Now lets proceed!"
      ],
      "metadata": {
        "id": "6QNovm-cfA9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we are using the TextVectorization layer\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\"\n",
        ")\n",
        "# Why in the next cell you are doing a different configuration of this layer."
      ],
      "metadata": {
        "id": "Cx2cKrQNJDwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "  lowercase_string = tf.strings.lower(string_tensor)\n",
        "  return tf.strings.regex_replace(\n",
        "      lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "# what does regex_replace method does?\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "  return tf.strings.split(string_tensor)\n",
        "\n",
        "\n",
        "# Now configuring the layer. I'm amazed to see that you have put custom functions in the parameters of the layer.\n",
        "  # Those parameters are 'standardize' and 'split'\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        "    )"
      ],
      "metadata": {
        "id": "687Hj4TLOnjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)\n",
        "\n",
        "# I think that this text_vectorization's method \"adapt\" is working similarly to the \"make_vocabulary\" method\n",
        "  # Ans: Actually the combination of adapt and get_vocabulary is working similar to make_vocabulary method"
      ],
      "metadata": {
        "id": "ZqOZRG51UpbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Displaying the vocabulary**"
      ],
      "metadata": {
        "id": "MrCo4nI6VjEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text_vectorization.get_vocabulary()\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)\n",
        "\n",
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi7tFNovUm3Z",
        "outputId": "9f2bbb61-171d-4a7b-de2b-b23a19cc521d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Uptill now, I know that the TextVectorization is imported from tensorflow.\n",
        "This parameters of this layer are customized by our own custom functions.\n",
        "\n",
        "How we are using it after the configuration?\n",
        "- Using this method: adapt(dataset)\n",
        "- Using this method: get_vocabulary()\n",
        "\n",
        "Then to encode any new data, we are just using the whole layer directly:\n",
        "-  text_vectorization(test_sentence)\n",
        "\n",
        "But we are decoding it by ourselves.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "2AY0bFpS9iTs",
        "outputId": "8c07ec0c-9661-4922-8a6f-a4e6f901937d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUptill now, I know that the TextVectorization is imported from tensorflow.\\nThis parameters of this layer are customized by our own custom functions.\\n\\nHow we are using it after the configuration?\\n- Using this method: adapt(dataset)\\n- Using this method: get_vocabulary()\\n\\nThen to encode any new data, we are just using the whole layer directly:\\n-  text_vectorization(test_sentence)\\n\\nBut we are decoding it by ourselves.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Decoding differences:\n",
        "our own layer: [2, 3, 5, 7, 1, 5, 6]\n",
        "tensorflow: [ 7  3  5  9  1  5 10]\n",
        "\n",
        "But the output is same\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "drKRDOHd9iWa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d6275c94-b6f6-42fd-bc98-21c6b0b4409f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDecoding differences:\\nour own layer: [2, 3, 5, 7, 1, 5, 6]\\ntensorflow: [ 7  3  5  9  1  5 10]\\n\\nBut the output is same\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two approaches for representing groups of words: Sets and sequences"
      ],
      "metadata": {
        "id": "S6NrphBdGy3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing the IMDB movie reviews data**"
      ],
      "metadata": {
        "id": "g09-oKXGG1hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee7AsMs6FmnG",
        "outputId": "6fb6dda8-3f36-4c33-f75c-e3de7873fb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  26.7M      0  0:00:03  0:00:03 --:--:-- 26.7M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "id": "PY6QMiyvHC6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fef1f98-c78b-4158-c057-565af9e1688b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category)\n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples = int(0.2 * len(files))   # I think: taking 20% validation data\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:                  # Moving data to val_dir from train_dir\n",
        "    shutil.move(train_dir / category / fname,\n",
        "                val_dir / category / fname)"
      ],
      "metadata": {
        "id": "WuCTxA1BHveA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For debugging:\n",
        "#shutil.rmtree(\"aclImdb/val/neg\", ignore_errors=True)\n",
        "#shutil.rmtree(\"aclImdb/val/pos\", ignore_errors=True)\n",
        "\n",
        "# Nothing more fearful than a powerful error.\n",
        "  # Fight hard!"
      ],
      "metadata": {
        "id": "h9oIARBFYOlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rough Work:\n",
        "#num_val_samples #2500\n",
        "#len(files)      #12500\n",
        "#files[:100]     # all are txt files"
      ],
      "metadata": {
        "id": "XIwq7QlXar07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Why the test directory is formed?\n",
        "#Ans: I THINK:\n",
        "# test and the train directory was created previously,\n",
        "# when the archive was created.\n",
        "# We have only created 'val' directory,\n",
        "# Maybe you can clear this by referring the book."
      ],
      "metadata": {
        "id": "L0Lwb5c7atbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "IhCCtBqiar3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b093d7f2-672e-481d-cbfd-ca2a3bd1689f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I think Method utils.text_dataset_from_directory from keras considers text files only\n",
        "# type(train_ds)   # tensorflow.python.data.ops.dataset_ops.BatchDataset      # rough\n",
        "# Will we use the all three directories?\n",
        "   #  Ans: Yes"
      ],
      "metadata": {
        "id": "A3A3AlkZF8dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rough Work\n",
        "#output of \"train_ds\"\n",
        "#<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ],
      "metadata": {
        "id": "r12n8kzHH_Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each iteration will being \"one batch\" of 32 text files\n",
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgfz_CgmF8ht",
        "outputId": "15ba9f41-2651-49ee-c164-c63c6bcbdc2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'The movie is wonderful. It shows the man\\'s work for the wilderness and a natural understanding of the harmony of nature, without being an \"extreme\" naturalist. I definitely plan to look for the book. This is a rare treasure!<br /><br />', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MY TESTING:\n",
        "#print(inputs[32]) # error in this test code\n",
        "# targets   # array of 1's and 0's\n",
        "#len(inputs[0]) # error\n",
        "#inputs[0].dtype # tf.string\n",
        "#a = tf.strings.length(inputs[:]).numpy() # 1270\n",
        "#print(a, max(a), min(a),sep='\\n')"
      ],
      "metadata": {
        "id": "kNXZ2TtcH-Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing words as a set: The bag-of-words approach"
      ],
      "metadata": {
        "id": "rMwJEiINUl2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Yes! I will not do as a sequence. Because I'm interested in bag-of-words approach only due to a better accuracy."
      ],
      "metadata": {
        "id": "IQsPwPQKVZdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ],
      "metadata": {
        "id": "BY2YKIeTUrFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing our datasets with a TextVectorization layer**"
      ],
      "metadata": {
        "id": "w3rqOvoCU5eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"multi_hot\"\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y:x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "_thfy0M3WF16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspecting the output of our binary unigram dataset:\n",
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeCuzh_9LDQb",
        "outputId": "60009c97-5d6a-4dee-ca9f-626730c5aaa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(inputs[0][:1000].numpy())   # length = 20000\n",
        "# Whole text data is now converted to 1's and 0's"
      ],
      "metadata": {
        "id": "PQwyznxGaioB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Model**"
      ],
      "metadata": {
        "id": "u0SEcxlMerQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "  inputs = keras.Input(shape=(max_tokens,))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\",\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "a3-i_uUra6EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVq9P8oEyc5C",
        "outputId": "27859056-f889-47ef-e6c9-76f2293fe649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing the binary unigram model**"
      ],
      "metadata": {
        "id": "QG07OHgcxGOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")              # Why loading the model now?\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t3JcRwXwtw3",
        "outputId": "a376d601-0e92-4a21-d24e-1b5b426b86da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 12s 19ms/step - loss: 0.4021 - accuracy: 0.8346 - val_loss: 0.2925 - val_accuracy: 0.8878\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2738 - accuracy: 0.8992 - val_loss: 0.2905 - val_accuracy: 0.8928\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2440 - accuracy: 0.9145 - val_loss: 0.3062 - val_accuracy: 0.8936\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2254 - accuracy: 0.9236 - val_loss: 0.3273 - val_accuracy: 0.8894\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2160 - accuracy: 0.9274 - val_loss: 0.3424 - val_accuracy: 0.8906\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2150 - accuracy: 0.9319 - val_loss: 0.3501 - val_accuracy: 0.8916\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2043 - accuracy: 0.9344 - val_loss: 0.3673 - val_accuracy: 0.8898\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2048 - accuracy: 0.9352 - val_loss: 0.3733 - val_accuracy: 0.8900\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2051 - accuracy: 0.9360 - val_loss: 0.3836 - val_accuracy: 0.8912\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2101 - accuracy: 0.9369 - val_loss: 0.3886 - val_accuracy: 0.8952\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.2883 - accuracy: 0.8868\n",
            "Test acc: 0.887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MY TESTING:\n",
        "#print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)}\")\n",
        "\n",
        "# MODEL TRAINING TIME WITHOUT GPU = ~ 1min\n",
        "# TEST ACCURACY: 88.7%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-iImhm1wt1V",
        "outputId": "f7c12a34-1512-40ef-daca-735e412842aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 12s 15ms/step - loss: 0.2883 - accuracy: 0.8868\n",
            "Test acc: [0.2882535457611084, 0.8868399858474731]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what is the purpose of \".cache\" in the above code cell"
      ],
      "metadata": {
        "id": "dwWHKNoN5WTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bigrams with binary encoding"
      ],
      "metadata": {
        "id": "-MNFL9j785kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing/Configuring the TextVectorization layer to return bigrams\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ],
      "metadata": {
        "id": "4TYw0Yhs5WWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing the binary bigram model**"
      ],
      "metadata": {
        "id": "_fpzu3V997wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "8WwbFP_Q-N5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ9PApgS9dRM",
        "outputId": "415951a0-0809-46d3-dedf-14b7152a4e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "P1PFNfmU-9Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST ACCURACY: 89.6%\n",
        "# MODEL TRAINING AND TESTING TIME ~ 1.5min "
      ],
      "metadata": {
        "id": "vr74dKw1_7Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I'M CHANGING EPOCHS\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=4,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ISdRXDnA8MG",
        "outputId": "0c1cfd0a-7068-4056-e841-6bde8405a2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "625/625 [==============================] - 14s 21ms/step - loss: 0.3707 - accuracy: 0.8476 - val_loss: 0.2718 - val_accuracy: 0.8928\n",
            "Epoch 2/4\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2434 - accuracy: 0.9145 - val_loss: 0.2779 - val_accuracy: 0.8946\n",
            "Epoch 3/4\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2059 - accuracy: 0.9323 - val_loss: 0.3000 - val_accuracy: 0.8936\n",
            "Epoch 4/4\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1900 - accuracy: 0.9425 - val_loss: 0.3126 - val_accuracy: 0.8982\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.2659 - accuracy: 0.8994\n",
            "Test acc: 0.899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST ACCURACY: 89.7%\n",
        "# MODEL TRAINING AND TESTING TIME = 45s"
      ],
      "metadata": {
        "id": "MqfbDZDcCnCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bigrams with TF-IDF encoding"
      ],
      "metadata": {
        "id": "R4cjt1-1fgz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring the TextVectorization layer to return token counts\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ],
      "metadata": {
        "id": "U_jEVSECfOOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring TextVectorization to return TF-IDF-weighted outputs\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")\n",
        "\n",
        "# Why overwriting?"
      ],
      "metadata": {
        "id": "JLG2ZUKxfORh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_only_train_ds = train_ds.map(lambda x, y:x)"
      ],
      "metadata": {
        "id": "u9JMekJZjlVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ],
      "metadata": {
        "id": "6SSw39dcgLQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrRx3_pkfOUV",
        "outputId": "10b68f0a-c19a-4983-9fff-e20e62aeefcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 21ms/step - loss: 0.5014 - accuracy: 0.7713 - val_loss: 0.3697 - val_accuracy: 0.8432\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.3431 - accuracy: 0.8580 - val_loss: 0.3270 - val_accuracy: 0.8732\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3052 - accuracy: 0.8710 - val_loss: 0.4143 - val_accuracy: 0.8522\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2932 - accuracy: 0.8781 - val_loss: 0.3530 - val_accuracy: 0.8832\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2771 - accuracy: 0.8889 - val_loss: 0.3456 - val_accuracy: 0.8664\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2630 - accuracy: 0.8932 - val_loss: 0.3510 - val_accuracy: 0.8750\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2653 - accuracy: 0.8913 - val_loss: 0.3708 - val_accuracy: 0.8676\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2561 - accuracy: 0.8968 - val_loss: 0.3912 - val_accuracy: 0.8736\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2487 - accuracy: 0.8975 - val_loss: 0.4054 - val_accuracy: 0.8814\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2349 - accuracy: 0.9033 - val_loss: 0.3888 - val_accuracy: 0.8594\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.3124 - accuracy: 0.8777\n",
            "Test acc: 0.878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST ACCURACY: 87.8%\n",
        "# MODEL TRAINING AND TESTING TIME ~ 2min"
      ],
      "metadata": {
        "id": "b68Rv_ARharr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "# \"binary_2gram.keras\"   \"tfidf_2gram.keras\""
      ],
      "metadata": {
        "id": "hYS9vcHepuDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(1,),dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "AftGPBJRhax6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([[\"This movie was a great one\"],])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0]*100):.2f} percent positive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opspVlHwlFvN",
        "outputId": "607b2cdf-4b50-4565-8acf-11698caa0cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62.60 percent positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING:\n",
        "# Book Review:   \"That was an excellent movie, I loved it.\"   95%\n",
        "# My review 1:   \"Crazy\"    49%\n",
        "# My review 2:   \"I liked the story and all the characters\"    77%\n",
        "# My review 3:   \"Happy ending!\"    57%\n",
        "# \"The story was a rubbish\" 39%\n",
        "# \"I hate this movie\" 45%\n",
        "# \"I did not find this movie interesting\" # 45%\n",
        "# \"I found this movie interesting\" # 50%\n",
        "# \"One of the best characters were acting in this movie and the story was very interesting\" # 62#\n",
        "#\"Awesome\" , \"Great\" 58%\n",
        "# \"This movie was a great one\"# 63%"
      ],
      "metadata": {
        "id": "9w42VW8NlGHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I will run TD-IDF model again and bring 95% accuracy again and test other reviews also.."
      ],
      "metadata": {
        "id": "0pzewB9nwbu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALHAMDULILLAH!"
      ],
      "metadata": {
        "id": "gE3YYfQEwkC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SUMMARY**"
      ],
      "metadata": {
        "id": "q9Dfwq8o6TTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Processing words as a “set”: The bag-of-words approach**\n",
        "\n",
        "1.   Binary Unigram Model\n",
        "2.   Binary Bigram Model\n",
        "3.   TF-IDF Bigram Model"
      ],
      "metadata": {
        "id": "SK4iliVx6jGO"
      }
    }
  ]
}